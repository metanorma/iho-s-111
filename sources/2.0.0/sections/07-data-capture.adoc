
[[sec_7]]
==  Data Capture and Classification

The Surface Current product contains data processed from sensors or
derived from the output from mathematical models. In most cases, the
data collected by the Producing Authority must be translated, sub-setted,
reorganized, or otherwise processed to be made into a usable data
format.

[[sec_7.1]]
=== Data sources

Surface current data comes primarily from a few specific sources:
observations, astronomical predictions, analyses, and forecast models.
When such data are produced and quality-controlled by an approved
Producing Authority (IHO Resolutions A6.3 & A6.9, <<S_62>>), they
are suitable for inclusion in the Surface Current data product. See
<<annex-e,style=full%>> -- Surface Current Data.

*Observational Data:*:: Observational surface current data comes initially
from _in situ_ sensors in the field (for example current meters or
drifting platforms) or from high-frequency radar, and such sensors
are monitored by the data collecting authority. After reception, the
data are quality-controlled and stored by the Producing Authority.
Some of the observed data may be available for distribution within
minutes of being collected and are thus described as being in real
time. Other data may be days or years old, and are called historical
data.

*Astronomical Predictions:*:: Astronomical predictions are produced
when a sufficiently long time series of observed currents has been
obtained and the data has been harmonically analysed by the Producing
Authority to produce a set of amplitude and phase constants. There
may be a single set of constants to represent flood and ebb currents
along a principal direction, or two sets of constants to represent
the northward and eastward components of the current. The harmonic
values can then be used to predict the astronomical component of the
current as a time series covering any desired time interval. In addition,
the harmonic constants may be used to estimate tidal currents for
a generic tidal cycle, with the specific amplitude and direction of
the current based on the tide range at a specified nearby tide station,
and the specific phase of the current based on the time of high water
at the same nearby tide station. Data such as these may be available
for single stations or, if the stations are numerous, they may be
arranged by the Producing Authority into a gridded field or a tidal
atlas.

*Analysed and Hybrid Values:*:: Analysed current values may be produced
from sea-surface topography, data assimilation, statistical correlations,
or other means. A hybrid method combines two or more approaches.

*Hindcast and Forecast Data:*:: Hydrodynamic models numerically solve
a set of fluid dynamic equations in two or three dimensions, and rely
on observational data, including water levels and winds, to supply
boundary conditions. Model grids may be either regular or ungeorectified.
Such models are often run several times per day, and in each run there
is usually a hindcast and a forecast. The hindcast is a model simulation
that attempts to recreate present conditions by using the most recent
observational data, while a forecast is a simulation made for many
hours into the future using predicted winds, water levels, etc. The
results are saved for a limited number of times, and are stored as
arrays that derive from the model's grid. These models and methods
are developed, run, and monitored by the Producing Authority.

These descriptions are summarized in <<table_7-1>>. (Note that the
encoding format does not designate observation data as "historical"
or "real-time".)

[[table_7-1]]
.Types of surface current data, based on the sources of the data
[cols="a,a,a"]
|===
h| Type h| Name h| Description
| 1 | Historical observation | Observation made hours, days, etc, in the past
| 2 | Real-time observation  | Observation no more than a few minutes old
| 3 | Astronomical prediction
| Value computed using harmonic analysis or other proven method of
tidal analysis [IHO Res. 3/1919, as amended]
| 4 | Analysis or hybrid method
| Calculation by statistical or other indirect methods, or a combination
of methods
| 5 | Hindcast
| Gridded data from a two- or three-dimensional dynamic simulation
of past conditions using only observed data for boundary forcing,
via statistical method or combination
| 6 | Forecast
| Gridded data from a two- or three-dimensional dynamic simulation
of future conditions using predicted data for boundary forcing, via
statistical method or combination

|===

[[sec_7.2]]
=== The production process

Nearly all available information on surface currents available from
the HO must be reformatted to meet the standards of this Product Specification
(<<sec_10>>). This means (a) populating the Carrier Metadata blocks
(<<sec_12.3>>) with the relevant data and (b) reorganizing the speed
and direction data when using the encoding rules (see <<sec_10>>).

[[sec_7.2.1]]
==== Metadata

Metadata is derivable from the information available from the approved
Authority. Recall that the definition of uncertainty (<<sec_1.3.2>>)
is based on the 95% confidence level. The following variables may
require additional processing:

* The bounding rectangle is computable from either the distribution
of stations or nodes, or from grid parameters.  

* Position uncertainties may be available from the approved Authority's
metadata; otherwise they must be calculated.

* Speed and direction uncertainties, if specified as a single value
for the dataset, may be available from the approved Authority; otherwise
they must be calculated

* If a previously issued data file is being cancelled or replaced,
the _replacedData_ and/or _dataReplacement_ attributes in the exchange
catalogue must be populated.

All mandatory metadata in carrier metadata (<<sec_12.3>>) must be
populated with appropriate values. In cases where the attribute is
mandatory but inapplicable, the appropriate fill or null value described
in <<sec_12.3>> must be used.

Similarly, when the Exchange Set is being compiled, all mandatory
metadata or information fields in the discovery metadata and Exchange
Catalogue (<<sec_12.1;and!sec_12.2>>) must be populated. In cases
where the attribute is mandatory but inapplicable, or the value is
unknown or not included in the relevant enumeration list, the appropriate
fill or null value described must be used.

NOTE: (informative): Running the S-100 level validation checks and
product-specific validation checks should detect missing metadata,
but as of May 2024 the checks are yet to be completely defined and
automated, and visual checking of metadata may be necessary. The Tables
in <<sec_12.2;and!sec_12.3>> describe the mandatory requirements and
allowed values.

[[sec_7.2.2]]
==== Surface current data

Observational currents and astronomical tidal current predictions
at a single location and gridded forecast data must normally be reformatted
to fit the S-111 Standard. The following may require additional calculations:

* Current depth values for modelled data grid points and for observational
data (such as for moored current meters) may require re-referencing
to a different vertical datum.
* For gridded data, if a land mask array is included, the mask value
is substituted into the gridded values as appropriate.
* Time stamps, if given in local time, must be converted to UTC.

[[sec_7.2.3]]
==== Digital tidal atlas data

Tidal atlas information may require additional processing to produce
a time series. A tidal atlas typically contains speed and direction
information for a number of locations, the valid time of which is
expressed as a whole number of hours before and after time of high
water, or current flood, at a reference tidal water level station
(<<table_F-1>>). The speed and direction for any time are computed
as a function of the daily predicted tides or currents at the reference
station. The conversion into a time series is the responsibility of
the Producing Authority.

[[sec_7.2.4]]
==== Validation

Dataset and Exchange Set validation tests must be passed before the
Exchange Set is published.

For numeric attributes, the fill value will be outside the allowed
range of values specified in the Feature Catalogue, if any. Similarly,
for enumerations, the fill value will not be a member of the enumeration
as listed in the Feature Catalogue. Validation checks for datasets
must allow for the presence of fill values.

Validation must apply both the S-100 level validation checks defined
in the S-100 validation specification (only those checks applicable
to S-111 need be applied) and the product-specific validation checks
provided in <<S_158_111>>.

[[sec_7.2.5]]
==== Digital signatures

Digital signatures are required for datasets and exchange sets intended
for use on ECDIS. <<S_100,part=15>> describes the required signature
algorithm and procedure for creating signatures. <<S_100,part=17>>
describes where signatures must be provided. Additional guidance common
to all datasets and exchange sets intended for ECDIS is being developed
by the IHO. In the absence of this common guidance, the following
guidance applies to S-111 datasets and exchange sets:

* The signature algorithm must be as specified in <<S_100,part=15>>.
* In discovery metadata, the *S100_SE_SignatureOnData* element should
be used to encapsulate digital signatures for datasets, with the _dataStatus_
attribute set to _unencrypted_ or _encrypted_ according to whether
the signature is for an unencrypted or encrypted HDF5 file.
* All resources in the exchange set must be signed, including any
catalogue(s) and support files.
* At least one signature is required for each resource (dataset, catalogue,
or support file) in the exchange set (the ECDIS will ignore unsigned
resources or resources for which signature verification fails).
* Additional signatures may optionally be provided, or added downstream
in the distribution chain, as provided for in S-100 Parts 17 and 15.

[[sec_7.3]]
=== Guidance for chunking and compression (informative)

Chunking affects both dataset size and optimised data retrieval, the
latter in the sense of how an ECDIS would most efficiently retrieve
relevant chunks of a dataset when a user pans and zooms.

Product Specification developers may desire to assess typical profiles
and volumes of data for their datasets and develop guidance for the
use of chunking and compression in their data products. Common practice
is provided below. Product teams should assess its applicability to
their own products and use, omit, and adapt it accordingly.

The development of guidance on how to optimally and correctly do chunking
and compression is ongoing; however, current best practice is:

* For gridded data with 2 dimensions, for example _dataCodingFormat_ = 2
(regular grids), choosing roughly-square rectangular chunk sizes will
result in better performance when reading subsets of the data, and
will probably result in better compression (one reason being that
because *NoData* areas tend to be clustered together geographically,
geographically-tiled chunks will compress out all those repetitive
values).
* Producers may use "auto-chunking", where this functionality is available
(for example, in the production toolset's HDF5 library). Auto-chunking
will choose chunk sizes automatically.
* Choosing the right chunk sizes depends on the type of data and what
the use of chunking is trying to accomplish. Auto chunking is more
ideal for compression and is less ideal for time-critical access patterns.

Auto-chunking means different datasets may be chunked differently.
Applications cannot expect a standardised chunk size and will have
to handle whatever chunk sizes they encounter in datasets.

Data Producers should note experiences from preliminary testing (on
water level data (S-104), but which should also apply to surface current
data):

* 2D arrays - Need to be chunked based on how the data is read. If
applications need to hold the entire grid in memory, use no chunking;
otherwise estimate a reasonable size for data extraction. It is probably
better to have the chunking set a little smaller than to make it too
big, for I/O purposes.
* 1D arrays -- Do not chunk unless they are enormous (for S-111 this
is not an issue since <<sec_11.2.1>> limits datasets to well below
the size where chunking matters).
* Given the relatively small sizes of datasets for S-111 (for example,
stem:[10 "unitsml(Mbyte_B)"] limit guidelines in <<sec_11.2.1>>)
chunking will not be of great benefit in read performance for S-111.

Producers should determine the compression scheme that is optimal
for their own use case, as needed.

[[sec_7.4]]
=== Datasets in a series

Datasets in a time series (for example, 4X daily, 1X daily, etc.)
may be distributed by any appropriate means, such as transfer to an
accessible Internet service or via a licensed distribution channel.

Each release by the producer should be accompanied by an exchange
catalogue and bear the appropriate producer digital signatures as
specified in <<S_100,part=17>> and <<S_98>>.

Route monitoring applications require up-to-date information and periodic
forecasts should be issued in a timely manner (meaning, a successor
dataset should be released before the expiry of one full period after
the starting date and time of its predecessor).

Multi-pack exchange sets containing multiple sequential datasets may
also be prepared as determined necessary by the producer, for example,
for uses other than route monitoring on ECDIS. For multi-packs a single
exchange catalogue containing discovery metadata for all datasets
should be prepared.

[[sec_7.5]]
=== Data use purpose

[[sec_7.5.1]]
==== Datum requirements

Datasets intended for use in navigation must use the same CRS as the
underlying ENC. Particular care should be taken to ensure that the
horizontal datum is the same as the underlying ENC (with preference
for S-101 over S-57). The epoch of realization should be included
in this assessment.

NOTE: Conformant datums are a requirement for display on ECDIS, as
described in <<S_98>>.

[[sec_7.5.2]]
==== Spatial type recommendations

Forecast datasets (type = astronomicalPrediction, analysisOrHybrid,
hydrodynamicForecast) intended for use in navigation should be issued
as regular grids if possible and if sufficiently high-quality gridded
forecasts can be produced (regular grids being most suitable for water
level adjustment, cf. <<S_98>>, and under the presumption that co-located
current data would be desirable). Station-based forecasts must also
be issued if the quality of the data so produced is better than the
gridded product in the vicinity of a station (for example, if the
local uncertainty is lower than the gridded product, or in case of
anomalous local currents).

Observation datasets will usually be issued in one of the point formats
(DCF 1 or 8).

[[sec_7.5.3]]
==== Suitability for navigation

Datasets may be marked for use in navigation if the Producer is able
to consistently produce data of sufficiently high quality.

[[sec_7.5.4]]
==== Use purpose metadata

Datasets not intended for navigation purposes must have the discovery
metadata attribute _notForNavigation_ in the corresponding *S100_DatasetDiscoveryMetadata* block set to _true_.

Datasets intended for navigation must have the discovery metadata
attribute _notForNavigation_ in the corresponding *S100_DatasetDiscoveryMetadata* block set to _false._

[[sec_7.6]]
=== Compliance categories

Compliance categories are described in <<S_100,clause=4a-5.5>>. Datasets
intended for use on ECDIS must meet the requirements for category4
and the compliance category must be encoded accordingly.

[[sec_7.7]]
=== Compliance with <<S_98>>

<<S_98>> consists of a specification for visual interoperability (S-98
Main, S-98 Parts A/B/C/D, and S-98 Annexes A and B) and a specification
for harmonised display of S-100 products on ECDIS (<<S_98,annex=C>>).
The requirements for datasets to be compliant with each aspect of
interoperability are described below. Compliance to this edition of
S-111 is a fundamental requirement and will not be explicitly listed.

[[sec_7.7.1]]
==== Requirements for visual interoperability

S-111 datasets must satisfy the following requirement:

* The S-111 dataset uses the same horizontal CRS as an underlying
(or overlapping) S-101 ENC or S-104 dataset.

[[sec_7.7.2]]
==== Requirements for harmonised user experience

S-111 datasets must also comply with the requirements for harmonised
user experience:

* There must be no spatial overlap between S-111 datasets created
by the same producer.
* Temporal overlap is permitted only for datasets which are members
of the same temporal series, when a forecast for a specific period
is followed by a forecast for a later period. S-111 provides for a
dataset naming convention that distinguishes successive datasets in
a temporal series.
* Any checks for cross-compatibility of S-101/S-102/S-104 and S-111
datasets must also be satisfied. Cross-compatibility checks will be
defined in <<S_158_98>> (Validation Checks -- Interoperability).

[[sec_7.8]]
=== Vertical datums

S-111 datasets are expected to use only a single vertical datum.

[[sec_7.9]]
=== Construction of coverages

Grids should generally use the <<S_100,part=8>> and <<ISO_19123_2005>>
convention that grid data are nominally situated exactly at the grid
points defined by the grid coordinates. This convention makes the
grid points the "sample points", representing data over a neighborhood
extending a half-cell in each direction (<<S_100,part=8,clause=8.2.5.8>>).
If this convention is followed, the attribute _dataOffsetCode_ (<<sec_12.3.2>>)
should not be encoded.

In exceptional circumstances, producers may construct grids where
the "sample points" are located at the centres of grid cells, in which
case _dataOffsetCode_ must be encoded with value "5: Barycenter (centroid)
of cell" (<<sec_12.3.2>>).

NOTE: The concept of a "sample point" as representing data for a sample
space does not overrule any recommendation about interopolating or
not interpolating S-111 data.

Note that a grid with 100x100 cells will have stem:[101 xx 101] grid
points. See <<sec_10.2.2.7>> for the rules specifying the dimensions
of the values array for each convention.
